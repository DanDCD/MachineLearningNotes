{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Out-of-Sample Error and the No Free Lunch Theorem\n",
    "\n",
    "In this notebook, we will walk through the mathematics behind out-of-sample error and the No Free Lunch (NFL) Theorem. We will break down important equations and explore examples using Python to reinforce key concepts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-Sample Error\n",
    "\n",
    "The out-of-sample error for a learning algorithm \n",
    "$$\\mathcal{L}_a$$ \n",
    "is given by:\n",
    "\n",
    "$$\n",
    "E_{\\text{ote}}(\\mathcal{L}_a | X, f) = \\sum_{h} \\sum_{x \\in \\mathcal{X} - X} P(x) \\mathbb{I}(h(x) \\neq f(x)) P(h | X, \\mathcal{L}_a)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\n",
    "E_{\\text{ote}}(\\mathcal{L}_a | X, f) \n",
    "$ Is the out-of-sample error for a learning algorithm, given the training set and the ground truth.\n",
    "\n",
    "- $\n",
    "\\mathcal{X} - X\n",
    "$ Is the whole input space subtract the training set, which is therefore equivalent to all unseen data (i.e the test set).\n",
    "\n",
    "- $\n",
    "P(x)\n",
    "$ Is the probability of observing (unseen/test) sample $x$.\n",
    "\n",
    "- $\n",
    "\\mathbb{I}(h(x) \\neq f(x))\n",
    "$ Is the indicator function, which will be $0$ when our hypothesis (i.e. prediction) for unseen sample $x$ is equal to the ground truth, and $1$ otherwise.\n",
    "\n",
    "- $\n",
    "P(h | X, \\mathcal{L}_a)\n",
    "$ Is the probability of obtaining the hypothesis $h$ given the training data $X$ and the learning algorithm $\\mathcal{L}_a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example in Python\n",
    "Below is a very simple dummy calculation for the out-of-sample error using numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-sample error: 0.25\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example data: input space (X) and hypothesis h\n",
    "X = np.array([0, 1, 2, 3])\n",
    "X_train = np.array([0, 1])  # Training set\n",
    "\n",
    "# Probabilities for the data points\n",
    "P_x = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "# True function f and hypothesis h\n",
    "f = np.array([0, 1, 1, 0])  # Target function\n",
    "h = np.array([0, 1, 0, 0])  # Hypothesis\n",
    "\n",
    "# Indicator function: 1 if h(x) != f(x), else 0\n",
    "indicator_function = (h != f).astype(int)\n",
    "\n",
    "# Out-of-sample error: consider only points not in the training set\n",
    "X_out_of_sample = np.setdiff1d(X, X_train)\n",
    "out_of_sample_error = np.sum(P_x[X_out_of_sample] * indicator_function[X_out_of_sample])\n",
    "\n",
    "print(f\"Out-of-sample error: {out_of_sample_error}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring NFL using Binary Classification problems\n",
    "If we consider a binary classification problem, where our labels/outputs are either $1$ or $0$, we can view $f$ (our ground truth) as any function $x\\rarr\\{0, 1\\}$.\n",
    "\n",
    "This will have a function space of $\\{0, 1\\}^{|\\mathcal{X}|}$, which expands to $\\{(0, 0, 0, 0, ...), (1, 0, 0, 0, ...), ...\\}$, where each tuple is a unique possible combination of outputs for the test data.\n",
    "\n",
    "Using combanatorics, we know that the function space  $\\{0, 1\\}^{|\\mathcal{X}|}$, must have size $2^{|\\mathcal{X}|}$, as have $|\\mathcal{X}|$ samples and $2$ possible choices for each sample, giving $2^{|\\mathcal{X}|}$ possible combinations of label choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All possible label combinations (function space):\n",
      "[[0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [1 1 0 0]\n",
      " [0 0 1 0]\n",
      " [0 1 1 0]\n",
      " [1 0 1 0]\n",
      " [1 1 1 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 1]\n",
      " [1 1 0 1]\n",
      " [0 0 1 1]\n",
      " [0 1 1 1]\n",
      " [1 0 1 1]\n",
      " [1 1 1 1]]\n",
      "\n",
      "Number of combinations: 16\n",
      "Expected number of combinations (2^4): 16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Number of samples |x|\n",
    "n_samples = 4  # You can change this to any integer\n",
    "\n",
    "# Generate all possible label combinations (0 or 1) for the n_samples\n",
    "label_combinations = np.array(np.meshgrid(*[[0, 1]] * n_samples)).T.reshape(-1, n_samples)\n",
    "\n",
    "# Print the possible combinations\n",
    "print(\"All possible label combinations (function space):\")\n",
    "print(label_combinations)\n",
    "\n",
    "# Check the total number of combinations, should be 2^n_samples\n",
    "num_combinations = label_combinations.shape[0]\n",
    "expected_combinations = 2 ** n_samples\n",
    "\n",
    "print(f\"\\nNumber of combinations: {num_combinations}\")\n",
    "print(f\"Expected number of combinations (2^{n_samples}): {expected_combinations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uniform Distribution of Target Function\n",
    "If we assume $f$ to be uniformly distributed (randomly assigning $0$ or $1$ with $0.5$ probability), then half of the possible functions $f \\in \\{0, 1\\}^{|\\mathcal{X}|}$ will produce the correct label for a given input.\n",
    "\n",
    "This means $\\frac{1}{2}2^{|\\mathcal{X}|}$ samples will be incorrectly labeled (where $h(x) \\neq f(x)$).\n",
    "\n",
    "Meaning, we can simplify our out-of-sample error to:\n",
    "$$\n",
    "E_{\\text{ote}}(\\mathcal{L}_a | X, f) = \\frac{1}{2} 2^{|\\mathcal{X}|} \\sum_{h} \\sum_{x \\in \\mathcal{X} - X} P(x) P(h | X, \\mathcal{L}_a)\n",
    "$$\n",
    "\n",
    "By subtracting $1$ from $2^x$ we effectively half it, so we can simplify to:\n",
    "\n",
    "$$\n",
    "E_{\\text{ote}}(\\mathcal{L}_a | X, f) = 2^{|\\mathcal{X}| - 1} \\sum_{h} \\sum_{x \\in \\mathcal{X} - X} P(x) P(h | X, \\mathcal{L}_a)\n",
    "$$\n",
    "\n",
    "We can re-arrange the sums:\n",
    "\n",
    "$$\n",
    "E_{\\text{ote}}(\\mathcal{L}_a | X, f) =  2^{|\\mathcal{X}| - 1} \\sum_{x \\in \\mathcal{X} - X} P(x) \\sum_h P(h | X, \\mathcal{L}_a) \n",
    "$$\n",
    "\n",
    "Because $P(h | X, \\mathcal{L}_a)$ is a probability distribution over h (i.e. the chance of obtaining each possible hypothesis in h), summing for all h with $\\sum_h P(h | X, \\mathcal{L}_a)$ must give a total probability of $1$.\n",
    "So, we can simplify further into:\n",
    "\n",
    "$$\n",
    "E_{\\text{ote}}(\\mathcal{L}_a | X, f) = 2^{|\\mathcal{X}| - 1} \\sum_{x \\in \\mathcal{X} - X} P(x) \\cdot 1\n",
    "$$\n",
    "\n",
    "What we observe is our out-of-sample error for $\\mathcal{L}_a$ is completely independent of $\\mathcal{L}_a$.\n",
    "\n",
    "This means for two different learners, $\\mathcal{L}_a$ and $\\mathcal{L}_b$:\n",
    "$$\n",
    "E_{\\text{ote}}(\\mathcal{L}_a | X, f) = E_{\\text{ote}}(\\mathcal{L}_b | X, f)\n",
    "$$\n",
    "\n",
    "Which would imply our choice of learner does not make a difference in error. No matter how 'smart' or 'dumb' a learner is, it has the same error as any other learner?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
